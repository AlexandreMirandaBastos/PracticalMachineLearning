---
title: "Practical Machine Learning - Final Project"
author: "Alexandre Miranda Bastos"
date: "11 de março de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document is the Final Project for the MOOC "Practical Machine Learning"" from Johns Hopkins University. 
For this Final Project the scripts have been produced and tested on RSudio Version 1.0.136  and Windows 10.

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Data 

The training data for this project are available here: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv]

The test data are available here: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv]

The data for this project come from this source: [http://groupware.les.inf.puc-rio.br/har]. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

## Theoretical description of reasoning




## Lybraries

Libraries to use in that work:caret, randomForest, rpart, lattice and ggplot2.

```{r , echo=TRUE, eval=TRUE}
###install.packages("caret")
###install.packages("randomForest")
###install.packages("rpart")
###install.packages("lattice") 
###install.packages("ggplot2")
###install.packages("rpart.plot")
library(caret)
library(randomForest)
library(rpart)
library(lattice)
library(ggplot2)
library(rpart.plot)
```


## Getting and Cleaning Data

In this step the files were loaded. Adjustments were made in the data to correct missing or non-standard information.
The report don´t show some transformation in data because its requirements of text and visualization optimization. 


```{r , echo=TRUE, eval=TRUE}

# After saving both data sets into my working directory
# Some missing values are coded as string "#DIV/0!" or "" or "NA" - these will be changed to NA.
# We notice that both data sets contain columns with all missing values - these will be deleted.  
trainingpack <- read.csv("C:/Dev/R/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
testingpack <- read.csv('C:/Dev/R/pml-testing.csv', na.strings=c("NA","#DIV/0!", ""))

# Check dimensions for number of variables and number of observations
dim(trainingpack)
dim(testingpack)

# Delete columns with all missing values
trainingpack<-trainingpack[,colSums(is.na(trainingpack)) == 0]
testingpack <-testingpack[,colSums(is.na(testingpack)) == 0]

# Some variables are irrelevant to our current project: user_name, raw_timestamp_part_1, raw_timestamp_part_,2 cvtd_timestamp, new_window, and  num_window (columns 1 to 7). We can delete these variables.
trainingpack   <-trainingpack[,-c(1:7)]
testingpack <-testingpack[,-c(1:7)]

# and have a look at our new datasets:
dim(trainingpack)
dim(testingpack)
```

```{r , echo=TRUE, eval=FALSE}
head(trainingpack)
head(testingpack)
```


## Exploratory Data Analysis

```{r , echo=TRUE, eval=FALSE}
# and have a look at our new datasets:
summary(trainingpack)
dim(trainingpack)
head(trainingpack)
summary(testingpack)
dim(testingpack)
head(testingpack)
```


## Reproducible Research

For the development of this work it was necessary to load some libraries of the R package. We used the following libraries: caret, randomForest, rpart, lattice, ggplot2, rpart.plot. 
 overall pseudo-random number generator seed was set at *5432* for all code. In order to reproduce the results below, the same seed should be useAnd.
 
```{r , echo=TRUE, eval=FALSE} 
# setting the overall seed for reproduceability
set.seed(5432)
```

## Regression Models

#### How We Decided the Best Model

The choice of variable "class" as variable outcome is a categorical variable with 5 classifications. The generation of the data counted on the participation of those involved to perform the movements with the dumbbell. The attributes of the class variable (A..E) take into account the specified movement (A) and the possible errors considered in the movement, namely: throw the elbows forward (B), raise the dumbbell only in the middle (C) , Lower the dumbbell only in the middle (D), throwing the hips forward (E).
Two models will be used for testing through decision tree and random forest algorithms. The forecast will propose to maximize accuracy and minimize out-of-sample error. After data cleansing, the remaining variables will also be used in the forecast.

#### Cross-validation

We considered the use of two subsets of data for the training data, redistributing it into "sub" training data (80% of the training data) and "sub" testing data (20% remaining). The models will be tested on the subtraining set and tested on the subtesting data. The highest precision model will be tested on the originally chosen test data.

```{r , echo=TRUE, eval=TRUE} 
# The training data set is partionned into 2 sets: subTraining (80%) and subTest (20%).
# This will be performed using random subsampling without replacement.

subpacks <- createDataPartition(y=trainingpack$classe, p=0.8, list=FALSE)
subTraining <- trainingpack[subpacks, ] 
subTesting <- trainingpack[-subpacks, ]
dim(subTraining)
dim(subTesting)

```
```{r , echo=TRUE, eval=FALSE} 
head(subTraining)
head(subTesting)
```

#### A look at the Data

The variable "classe" contains 5 levels: A, B, C, D and E. A plot of the outcome variable will allow us to see the frequency of each levels in the subTraining data set and compare one another.

 
```{r , echo=FALSE, eval=TRUE} 
plot(subTraining$classe, col="GREEN", main="Bar Plot of levels of the variable classe within the subTraining data set", xlab="classe levels", ylab="Frequency")
```




#### Expected out-of-sample error

Since Accuracy ~ (Correct Classified Observation) / (Samples(Subtesting Data Set)),

And expected accuracy is the expected accuracy in the out-of-sample data set, then the expected value of the out-of-sample error will correspond to:

     Number of Missclassified Observations/Samples(Test Data Set), that is the quantity ( 1-accuracy found from the cross-validation data set).

## First prediction model - Decision Tree

```{r , echo=FALSE, eval=TRUE} 
model1 <- rpart(classe ~ ., data=subTraining, method="class")

# Predicting:
prediction1 <- predict(model1, subTesting, type = "class")

# Plot of the Decision Tree
rpart.plot(model1, main="Classification Tree", extra=105, under=TRUE, faclen=10)
```

```{r , echo=TRUE, eval=TRUE} 
# Test results on our subTesting data set:
confusionMatrix(prediction1, subTesting$classe)
```

## Second prediction model: Using Random Forest

```{r , echo=TRUE, eval=TRUE} 
model2 <- randomForest(classe ~. , data=subTraining, method="class")
# Predicting:
prediction2 <- predict(model2, subTesting, type = "class")
# Test results on subTesting data set:
confusionMatrix(prediction2, subTesting$classe)
```

## Conclusion

Analyzing the following information:
  Accuracy (Random Forest): 0.9985 (95% CI: (0.9967, 0.9994))
  Accuracy (Decision Tree): 0.7489 (95% CI: (0.735, 0.7624))
So we decided on the random forest model with its precision of 0.9985. The expected error outside the sample is calculated by (1 - Accuracy) in the predictions made on the cross validation set. We conclude that the expected error is = 0.0015 (0.15%).

## Submission to Coursera

```{r , echo=TRUE, eval=TRUE}
# predict outcome levels on the original Testing data set using Random Forest algorithm
bestModel <- predict(model2, testingpack, type="class")
bestModel
```

#### Write files for submission

```{r , echo=TRUE, eval=TRUE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(bestModel)
```

